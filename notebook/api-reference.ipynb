{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# News-Watch API Reference\n\nThis notebook demonstrates all the key functions in the news-watch Python API with practical examples.\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/okkymabruri/news-watch/blob/main/notebook/api-reference.ipynb)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "First, install news-watch and its dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install news-watch\n",
    "!pip install news-watch\n",
    "!playwright install chromium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import newswatch as nw\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\nprint(\"News-watch API Reference\")\nprint(\"=\" * 40)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Functions\n",
    "\n",
    "### 1.1 list_scrapers() - Get Available News Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of all available news sources\n",
    "available_scrapers = nw.list_scrapers()\n",
    "print(\"Available news sources:\")\n",
    "for i, scraper in enumerate(available_scrapers, 1):\n",
    "    print(f\"  {i:2d}. {scraper}\")\n",
    "\n",
    "print(f\"\\nTotal: {len(available_scrapers)} news sources\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 scrape() - Basic Article Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic scraping - returns list of article dictionaries\n",
    "articles = nw.scrape(\n",
    "    keywords=\"ekonomi\",\n",
    "    start_date=\"2025-01-15\",\n",
    "    scrapers=\"kompas\",  # Use single reliable source for demo\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"Found {len(articles)} articles\")\n",
    "\n",
    "# Show structure of first article\n",
    "if articles:\n",
    "    print(\"\\nFirst article structure:\")\n",
    "    sample_article = articles[0]\n",
    "    for key, value in sample_article.items():\n",
    "        print(f\"  {key}: {str(value)[:60]}{'...' if len(str(value)) > 60 else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 scrape_to_dataframe() - Get Results as pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get results as pandas DataFrame for analysis\n",
    "df = nw.scrape_to_dataframe(\n",
    "    keywords=\"teknologi,digital\",\n",
    "    start_date=\"2025-01-15\",\n",
    "    scrapers=\"detik,kompas\"\n",
    ")\n",
    "\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "if not df.empty:\n",
    "    print(\"\\nDataFrame info:\")\n",
    "    print(df.info())\n",
    "    \n",
    "    print(\"\\nFirst 3 rows:\")\n",
    "    print(df.head(3)[['title', 'source', 'publish_date']].to_string())\n",
    "    \n",
    "    print(\"\\nSource distribution:\")\n",
    "    print(df['source'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 scrape_to_file() - Save Results Directly to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save directly to Excel file\n",
    "nw.scrape_to_file(\n",
    "    keywords=\"pendidikan\",\n",
    "    start_date=\"2025-01-15\",\n",
    "    output_path=\"education_news.xlsx\",\n",
    "    output_format=\"xlsx\",\n",
    "    scrapers=\"tempo,antaranews\"\n",
    ")\n",
    "\n",
    "print(\"✅ Education news saved to education_news.xlsx\")\n",
    "\n",
    "# Save to CSV\n",
    "nw.scrape_to_file(\n",
    "    keywords=\"kesehatan\",\n",
    "    start_date=\"2025-01-15\",\n",
    "    output_path=\"health_news.csv\",\n",
    "    output_format=\"csv\",\n",
    "    scrapers=\"kompas\"\n",
    ")\n",
    "\n",
    "print(\"✅ Health news saved to health_news.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Convenience Functions\n",
    "\n",
    "### 2.1 quick_scrape() - Get Recent News Easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get recent news without specifying exact dates\n",
    "recent_politics = nw.quick_scrape(\n",
    "    keywords=\"politik\",\n",
    "    days_back=3,  # Last 3 days\n",
    "    scrapers=\"auto\"\n",
    ")\n",
    "\n",
    "print(f\"Found {len(recent_politics)} political articles from last 3 days\")\n",
    "\n",
    "if not recent_politics.empty:\n",
    "    print(\"\\nMost recent articles:\")\n",
    "    recent_sorted = recent_politics.sort_values('publish_date', ascending=False)\n",
    "    for _, article in recent_sorted.head(3).iterrows():\n",
    "        print(f\"  • {article['title'][:60]}... ({article['source']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 scrape_ihsg_news() - Specialized Stock Market News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Indonesian stock market (IHSG) specific news\n",
    "stock_news = nw.scrape_ihsg_news(days_back=5)\n",
    "\n",
    "print(f\"Found {len(stock_news)} IHSG-related articles from last 5 days\")\n",
    "\n",
    "if not stock_news.empty:\n",
    "    # Analyze sentiment words in titles\n",
    "    positive_words = ['naik', 'menguat', 'positif', 'bullish']\n",
    "    negative_words = ['turun', 'melemah', 'negatif', 'bearish']\n",
    "    \n",
    "    positive_count = stock_news['title'].str.contains('|'.join(positive_words), case=False).sum()\n",
    "    negative_count = stock_news['title'].str.contains('|'.join(negative_words), case=False).sum()\n",
    "    \n",
    "    print(f\"\\nSentiment analysis:\")\n",
    "    print(f\"  Positive sentiment indicators: {positive_count} articles\")\n",
    "    print(f\"  Negative sentiment indicators: {negative_count} articles\")\n",
    "    \n",
    "    print(\"\\nDaily IHSG news volume:\")\n",
    "    daily_counts = stock_news.groupby(stock_news['publish_date'].dt.date).size()\n",
    "    print(daily_counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}